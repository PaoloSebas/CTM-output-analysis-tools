{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63a6a2c4-0007-4db2-9cc2-e379bc02ee65",
   "metadata": {},
   "source": [
    "# Analysis of Surface Deposition Processes at MacQuarie Island"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619229d-cbed-45dc-9370-6ff7e7529ade",
   "metadata": {},
   "source": [
    "What are the processes?:\n",
    "\n",
    "- Dry Dep of Hg0\n",
    "- Dry Dep of all Hg(2+)\n",
    "- Dry Dep of all Hg(P)\n",
    "- Wet Dep of Hg0\n",
    "- Wet Dep of all Hg(2+)\n",
    "- Wet Dep of all Hg(P)\n",
    "- Loss of Hg2 to SSA (that will be deposited too)\n",
    "\n",
    "GEOS-Chem v14.6.0 <br>\n",
    "Spin-up: 4 years <br>\n",
    "Simulation period: 2018-2022 <br>\n",
    "GEOS-Chem setting: preindustrial <br>\n",
    "Info su Fires: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4c2a25-e2bd-4460-8a67-8be14f4a10b0",
   "metadata": {},
   "source": [
    "At the moment, the code consider a small piece of land in the MQI grid cell\n",
    "-> A new diagnostic for little islands is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5306d2c-0b6f-4179-b7f3-694baaee7899",
   "metadata": {},
   "source": [
    "The simulation outputs are in **data_dir = 'D:/MERCURY_ANU/DATA_ANALYSIS/v14_6_0/Preind_outputs/'** and they are organised by year and type of perturbations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ca93e6-7904-449f-8ce0-258292f70ed9",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9826769f-b08f-4b04-83cc-8eb3d16f32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import netCDF4\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import seaborn as sns\n",
    "import cartopy.feature as cfeature\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ad81a2-0d03-4b6f-86ee-cdb780e4d050",
   "metadata": {},
   "source": [
    "## Organising path, lists, maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e3482c-278d-4296-a51c-26362f355664",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'D:/MERCURY_ANU/DATA_ANALYSIS/v14_6_0/Preind_outputs/'\n",
    "years = ['2018', '2019', '2020', '2021', '2022']\n",
    "perturbation_types = ['NO_PERTUBS', 'OCEAN_LESS', 'OCEAN_MORE', 'SSA_CONSTANT', 'SSA_ZERO', 'WIND_LESS', 'WIND_MORE']\n",
    "data_types = ['DryDep', 'WetLossLS', 'WetLossConv', 'MercuryChem', 'MercuryEmis', 'MercuryOcean', 'SpeciesConc', 'StateMet', 'Budget']\n",
    "\n",
    "perturbation_type_map = {\n",
    "    'NO_PERTUBS': 'NP',\n",
    "    'OCEAN_LESS': 'OL',\n",
    "    'OCEAN_MORE': 'OM',\n",
    "    'SSA_CONSTANT': 'SSAC',\n",
    "    'SSA_ZERO': 'SSA0',\n",
    "    'WIND_LESS': 'WL',\n",
    "    'WIND_MORE': 'WM'\n",
    "}\n",
    "\n",
    "data_type_map = {\n",
    "    'DryDep': 'DD',  # <<<<\n",
    "    'WetLossLS': 'WLLS',  # <<<<\n",
    "    'WetLossConv': 'WLConv', # <<<<\n",
    "    'MercuryChem': 'HgChem',  # <<<<\n",
    "    'MercuryEmis': 'HgEmis',\n",
    "    'MercuryOcean': 'HgOcean',\n",
    "    'SpeciesConc': 'SpCon',\n",
    "    'StateMet': 'Met',  # <<<<\n",
    "    'Budget': 'Budget'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1573e0-b681-488e-81a1-d120a3560536",
   "metadata": {},
   "source": [
    "## FUNCTION 1 : get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba51853-1761-492e-8f37-114afd965656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(year, type_name, data_type):\n",
    "    \"\"\"\n",
    "    Loading a dataset xarray specific according the provided year, pertubation_type, and data_type\n",
    "    \n",
    "    Args:\n",
    "        year (str): data year (es. '2018').\n",
    "        type_name (str): Simulation type (es. 'NO_PERTUBS').\n",
    "        data_type (str): Data type  (es. 'DryDep').\n",
    "\n",
    "    Returns:\n",
    "        xarray.Dataset: The loaded dataset, o None \n",
    "    \"\"\"\n",
    "    # Building the path\n",
    "    path_key = f\"{perturbation_type_map[type_name]}_{year}_{data_type_map[data_type]}\"\n",
    "    full_path = os.path.join(data_dir, year, type_name, f'GEOSChem.{data_type}.{year}*.nc4')\n",
    "    \n",
    "    print(f\"Files for {path_key}...\")\n",
    "    file_list = sorted(glob.glob(full_path))\n",
    "    \n",
    "    if not file_list:\n",
    "        print(f\"No files found {path_key}.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Loading {len(file_list)} file for {path_key}...\")\n",
    "    try:\n",
    "        ds = xr.open_mfdataset(file_list, combine='by_coords', parallel=True)\n",
    "        print(\"Loading Completed\")\n",
    "        return ds\n",
    "    except Exception as e:\n",
    "        print(f\"Loading error for the dataset in {path_key}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a755b68-68b5-456b-b6d7-7a9218a62633",
   "metadata": {},
   "source": [
    "## Constants and conversion factors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700909b6-af71-4e70-baac-e4b7e6c5f46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIME\n",
    "s_in_month = 2.628e6\n",
    "s_in_yr = 3.154e7 \n",
    "\n",
    "#MASS\n",
    "g_kg = 1e3\n",
    "ug_g = 1e6\n",
    "ug_kg = 1e9\n",
    "\n",
    "#AREA amd Volume\n",
    "cm2_m2 = 1e4\n",
    "cm3_m3 = 1e6\n",
    "\n",
    "#Chemistry \n",
    "MW_Hg = 200.59      ## <<< Mercury Molecular Weight\n",
    "NA = 6.023e23       ## <<< Avogadro's number\n",
    "\n",
    "# CONVERSION FACTOR which don't need inquiring the sim outputs\n",
    "cf_units_dd_month = (MW_Hg/NA) * ug_g * cm2_m2 * s_in_month    ### <<<  from molecule/cm2*s   to micrograms/m2*month\n",
    "cf_units_dd_year  = (MW_Hg/NA) * ug_g * cm2_m2 * s_in_yr       ### <<<  from molecule/cm2*s   to micrograms/m2*year\n",
    "\n",
    "conv = (MW_Hg/NA) * ug_g * 1e6 * s_in_yr\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69aaabab-03ec-4017-b491-0d61d9e00372",
   "metadata": {},
   "source": [
    "## Code useful for plotting purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3eb81f-d73a-4e44-abe2-e30a22a1ed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining maps' colors and line style \n",
    "\n",
    "colors = {\n",
    "    'NP': 'black', 'WM': 'blue', 'WL': 'lightblue',\n",
    "    'OM': 'green', 'OL': 'lightgreen', 'SSA0': 'red', 'SSAC': 'yellow'\n",
    "}\n",
    "\n",
    "linestyles = {\n",
    "    'NP': '--', 'WM': '-', 'WL': '-',\n",
    "    'OM': '-', 'OL': '-', 'SSA0': '-', 'SSAC': '-'\n",
    "}\n",
    "\n",
    "legend_map = {\n",
    "    'NP': 'No Perturbations',\n",
    "    'WM': 'More_Wind',\n",
    "    'WL': 'Less_Wind',\n",
    "    'OM': 'More_Ocean',\n",
    "    'OL': 'Less_Ocean',\n",
    "    'SSA0': 'Sea Salt Aerosols (Zero)',\n",
    "    'SSAC': 'Sea Salt Aerosols (Constant)'\n",
    "}\n",
    "\n",
    "month_names = ['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59060fac-dcd0-4a8b-ac9d-13f1d7a3434c",
   "metadata": {},
   "source": [
    "## Chemical species lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881196d-732e-4409-9188-8be3232bc2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hg2_species_list_sp = [\n",
    "    'SpeciesConcVV_HgCl2',\n",
    "    'SpeciesConcVV_HgOHOH',\n",
    "    'SpeciesConcVV_HgOHBrO',\n",
    "    'SpeciesConcVV_HgOHClO',\n",
    "    'SpeciesConcVV_HgOHHO2',\n",
    "    'SpeciesConcVV_HgOHNO2',\n",
    "    'SpeciesConcVV_HgClOH',\n",
    "    'SpeciesConcVV_HgClBr',\n",
    "    'SpeciesConcVV_HgClBrO',\n",
    "    'SpeciesConcVV_HgClClO',\n",
    "    'SpeciesConcVV_HgClHO2',\n",
    "    'SpeciesConcVV_HgClNO2',\n",
    "    'SpeciesConcVV_HgBr2',\n",
    "    'SpeciesConcVV_HgBrOH',\n",
    "    'SpeciesConcVV_HgBrClO',\n",
    "    'SpeciesConcVV_HgBrBrO',\n",
    "    'SpeciesConcVV_HgBrHO2',\n",
    "    'SpeciesConcVV_HgBrNO2'\n",
    "]\n",
    "\n",
    "hg2_species_list_DD = [    ##<<< They are 18 species\n",
    "    'DryDep_HgCl2',\n",
    "    'DryDep_HgOHOH',\n",
    "    'DryDep_HgOHBrO',\n",
    "    'DryDep_HgOHClO',\n",
    "    'DryDep_HgOHHO2',\n",
    "    'DryDep_HgOHNO2',\n",
    "    'DryDep_HgClOH',\n",
    "    'DryDep_HgClBr',\n",
    "    'DryDep_HgClBrO',\n",
    "    'DryDep_HgClClO',\n",
    "    'DryDep_HgClHO2',\n",
    "    'DryDep_HgClNO2',\n",
    "    'DryDep_HgBr2',\n",
    "    'DryDep_HgBrOH',\n",
    "    'DryDep_HgBrClO',\n",
    "    'DryDep_HgBrBrO',\n",
    "    'DryDep_HgBrHO2',\n",
    "    'DryDep_HgBrNO2'\n",
    "]\n",
    "\n",
    "## There are 18 from Hg(2+), 2 from Hg(P) ad 1 from Hg(0) => 21 variables for the total one \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce211f3d-c5b4-4ca8-b610-38b9bf8140bf",
   "metadata": {},
   "source": [
    "<h2 style=\"color: blue;\">DATA ANALYSIS</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b7410-83f1-4fa0-bc6a-13446341237e",
   "metadata": {},
   "source": [
    "Creating datasets which store values according to the pertubation type and the year <br>\n",
    "\n",
    "Name of the datasets: **datasets_by_year** \n",
    "\n",
    "They will be accessible using for example: \n",
    "\n",
    "datasets_by_year **['NP_2018_DD']**         <<< (It will contains the 12 months for the 2018, DD, NP case) \n",
    " \n",
    "where NP = No perturbations, and DD is DryDep. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c0370f-bf88-42fd-ad2c-f5c7fddac991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quindi da datasets_by_year['NP_2018_DD'] puoi recuperare DryDep_Hg0 per esempio\n",
    "# Quindi da datasets_by_year['NP_2018_SpCon'] puoi recuperare SpeciesConcVV_Hg0 per esempio\n",
    "# Quindi da datasets_by_year['NP_2018_HgChem'] puoi recuperare Hg2GasToSSA per esempio\n",
    "\n",
    "# A bit slow, but should be good\n",
    "\n",
    "data_types = ['SpeciesConc', 'DryDep', 'MercuryChem', 'StateMet']\n",
    "data_types_WD = ['WetLossLS', 'WetLossConv'] # Poiché questo blocco di codice si concentra solo su WetLoss\n",
    "\n",
    "datasets_by_year = {}\n",
    "datasets_by_year_WD = {}\n",
    "\n",
    "for y in years:\n",
    "    \n",
    "    for t in perturbation_types:\n",
    "        \n",
    "        for d in data_types:\n",
    "            \n",
    "            # Build the key dynamically\n",
    "            path_key = f\"{perturbation_type_map[t]}_{y}_{data_type_map[d]}\"\n",
    "            \n",
    "            # Load the dataset on-demand\n",
    "            dataset = get_dataset(year=y, type_name=t, data_type=d)\n",
    "            \n",
    "            # Save the dataset in the main dictionary only if it's valid\n",
    "            if dataset is not None:\n",
    "                datasets_by_year[path_key] = dataset\n",
    "\n",
    "        for e in data_types_WD:\n",
    "            \n",
    "            # Costruisci la chiave in modo dinamico\n",
    "            path_key_WD = f\"{perturbation_type_map[t]}_{y}_{data_type_map[e]}\"\n",
    "\n",
    "            # Carica il dataset on-demand\n",
    "            dataset_WD = get_dataset(year=y, type_name=t, data_type=e)\n",
    "\n",
    "            # Salva il dataset nel dizionario principale solo se è valido\n",
    "            if dataset_WD is not None:\n",
    "                datasets_by_year_WD[path_key_WD] = dataset_WD\n",
    "\n",
    "print(\"\\n--- Loading completed ---\")\n",
    "print(f\"{len(datasets_by_year)} datasets have been loaded.\")\n",
    "print(\"Dictionary keys are:\")\n",
    "print(datasets_by_year.keys())\n",
    "\n",
    "print(\"\\n--- Loading completed ---\")\n",
    "print(f\"{len(datasets_by_year_WD)} datasets have been loaded.\")\n",
    "print(\"Dictionary keys are:\")\n",
    "print(datasets_by_year_WD.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49dde74-f927-4fb4-8600-61414c0a8101",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(datasets_by_year['NP_2018_DD'].variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012c54a4-069f-459f-88a8-4965adea9ac2",
   "metadata": {},
   "source": [
    "There are 21 variables <= 2 Hg(P), 18  Hg(2+), 1  Hg(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48194ccd-397c-4092-b796-4149840655e0",
   "metadata": {},
   "source": [
    "Let's start with DD. Storing each contribution (PERTUBS_YEAR_DD datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9ed22-6260-46aa-af5a-155ec9c649b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each key-value pair in your dictionary\n",
    "\n",
    "for key, ds in datasets_by_year.items():\n",
    "    \n",
    "    # Only process datasets that end with '_DD'\n",
    "    \n",
    "    if key.endswith('_DD'):\n",
    "        print(f\"Processing dataset for key: {key}\")\n",
    "    \n",
    "        # 1. Calculate DryDep_Total\n",
    "        # We find all variables starting with 'DryDep_' but not 'DryDepVel_'\n",
    "        \n",
    "        all_drydep_vars = [var for var in ds.data_vars if 'DryDep_' in var and 'DryDepVel_' not in var] ##<< somma 22 variabili per NP, perche'?\n",
    "        \n",
    "        # Sum all selected DryDep DataArrays. xarray handles alignment automatically.\n",
    "        ds['DryDep_Total'] = ds[all_drydep_vars].to_array().sum('variable', skipna=False)\n",
    "    \n",
    "        # Add a descriptive attribute\n",
    "        ds['DryDep_Total'].attrs['long_name'] = 'Total Dry Deposition of all species'\n",
    "        \n",
    "        print(f\"  - DryDep_Total created. It is the sum of {len(all_drydep_vars)} variables.\")\n",
    "    \n",
    "        # 2. Calculate DryDep_Hg2\n",
    "        # Create a new DataArray by summing the specified species.\n",
    "        # Note: `ds[hg2_species_list]` returns a new Dataset with only these variables\n",
    "        # The .to_array().sum() converts the Dataset to a DataArray and sums over the new dimension.\n",
    "        \n",
    "        valid_hg2_species = [var for var in hg2_species_list_DD if var in ds.data_vars]\n",
    "        if valid_hg2_species:\n",
    "            ds['DryDep_Hg2'] = ds[valid_hg2_species].to_array().sum('variable', skipna=False)\n",
    "            ds['DryDep_Hg2'].attrs['long_name'] = 'Total Dry Deposition of Hg(2+) species'\n",
    "            print(f\"  - DryDep_Hg2 created. Summed {len(valid_hg2_species)} species.\")\n",
    "        else:\n",
    "            print(\"  - Warning: No valid Hg2+ species found in this dataset. Skipping DryDep_Hg2.\")\n",
    "    \n",
    "    \n",
    "        # 3. Calculate DryDep_HgP\n",
    "        # Find all variables that end with 'P'\n",
    "        hgp_species = [var for var in ds.data_vars if var.endswith('P') and 'DryDep_' in var and 'DryDepVel_' not in var]\n",
    "        \n",
    "        # Sum the selected variables\n",
    "        if hgp_species:\n",
    "            ds['DryDep_HgP'] = ds[hgp_species].to_array().sum('variable', skipna=False)\n",
    "            ds['DryDep_HgP'].attrs['long_name'] = 'Total Dry Deposition of Particulate species'\n",
    "            print(f\"  - DryDep_HgP created. Summed {len(hgp_species)} particulate species.\")\n",
    "        else:\n",
    "            print(\"  - Warning: No particulate species found in this dataset. Skipping DryDep_HgP.\")\n",
    "    \n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "# Find the first key ending in '_DD' to print a valid example\n",
    "first_dd_key = next((key for key in datasets_by_year.keys() if key.endswith('_DD')), None)\n",
    "\n",
    "if first_dd_key:\n",
    "    print(f\"\\nFinal variables for a processed dataset ({first_dd_key}):\")\n",
    "    print(list(datasets_by_year[first_dd_key].variables))\n",
    "    print(f\"\\nExample of new variable `DryDep_Total`:\")\n",
    "    print(datasets_by_year[first_dd_key]['DryDep_Total'])\n",
    "else:\n",
    "    print(\"\\nNo '_DD' datasets were found to process or print.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311450d0-4012-4aee-80d9-92101267d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datasets_by_year['NP_2018_DD']['DryDep_Total'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024884ba-a823-4fa4-a468-a01de22e483d",
   "metadata": {},
   "source": [
    "## Calculating the means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f003ed-06cb-428a-938b-b88c10c47203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops: Datarrays for each perturbation type \n",
    "data_arrays_by_type_DD = {u: [] for u in perturbation_type_map.values()}  # <<< total\n",
    "data_arrays_by_type_DD_Hg2 = {u: [] for u in perturbation_type_map.values()}\n",
    "data_arrays_by_type_DD_HgP = {u: [] for u in perturbation_type_map.values()}\n",
    "data_arrays_by_type_DD_Hg0 = {u: [] for u in perturbation_type_map.values()}\n",
    "\n",
    "# Dictionary to save the final monthly and annual mean values \n",
    "\n",
    "monthly_means_by_type_DD = {}\n",
    "annual_means_by_type_DD = {}\n",
    "\n",
    "monthly_means_by_type_DD_Hg2 = {}\n",
    "annual_means_by_type_DD_Hg2 = {}\n",
    "\n",
    "monthly_means_by_type_DD_HgP = {}\n",
    "annual_means_by_type_DD_HgP = {}\n",
    "\n",
    "monthly_means_by_type_DD_Hg0 = {}\n",
    "annual_means_by_type_DD_Hg0  = {}\n",
    "\n",
    "#### STANDARD DEVIATION\n",
    "\n",
    "monthly_std_by_type_DD = {}\n",
    "annual_std_by_type_DD = {}\n",
    "\n",
    "monthly_std_by_type_DD_Hg2 = {}\n",
    "annual_std_by_type_DD_Hg2 = {}\n",
    "\n",
    "monthly_std_by_type_DD_HgP = {}\n",
    "annual_std_by_type_DD_HgP = {}\n",
    "\n",
    "monthly_std_by_type_DD_Hg0 = {}\n",
    "annual_std_by_type_DD_Hg0 = {}\n",
    "\n",
    "###########################################################################\n",
    "# First loop: Datarrays for each perturbation type \n",
    "\n",
    "for t, t_key in perturbation_type_map.items():\n",
    "    for y in years:\n",
    "        full_key_DD = f\"{t_key}_{y}_DD\"\n",
    "\n",
    "        #### DRYDEP \n",
    "        \n",
    "        if full_key_DD in datasets_by_year:\n",
    "            data_array_DD = datasets_by_year[full_key_DD]['DryDep_Total']\n",
    "            data_arrays_by_type_DD[t_key].append(data_array_DD)\n",
    "        else:\n",
    "            print(f\"Warning: The key {full_key_DD} not found .\")\n",
    "        \n",
    "        if full_key_DD in datasets_by_year:\n",
    "            data_array_DD_Hg2 = datasets_by_year[full_key_DD]['DryDep_Hg2']\n",
    "            data_arrays_by_type_DD_Hg2[t_key].append(data_array_DD_Hg2)\n",
    "        else:\n",
    "            print(f\"Warning: The key {full_key_DD} not found .\")\n",
    "        \n",
    "        if full_key_DD in datasets_by_year:\n",
    "            data_array_DD_HgP = datasets_by_year[full_key_DD]['DryDep_HgP']\n",
    "            data_arrays_by_type_DD_HgP[t_key].append(data_array_DD_HgP)\n",
    "        else:\n",
    "            print(f\"Warning: The key {full_key_DD} not found .\")\n",
    "            \n",
    "        if full_key_DD in datasets_by_year:\n",
    "            data_array_DD_Hg0 = datasets_by_year[full_key_DD]['DryDep_Hg0']\n",
    "            data_arrays_by_type_DD_Hg0[t_key].append(data_array_DD_Hg0)\n",
    "        else:\n",
    "            print(f\"Warning: The key {full_key_DD} not found .\")\n",
    "\n",
    "#################################################################################\n",
    "# Second loop: Calculating the monthly and annual mean for each perturbation type \n",
    "\n",
    "###### DryDep_Total\n",
    "\n",
    "for t_key, data_arrays_list_DD in data_arrays_by_type_DD.items():\n",
    "    \n",
    "    if data_arrays_list_DD:\n",
    "        \n",
    "        combined_data_DD = xr.concat(data_arrays_list_DD, dim='time')\n",
    "        \n",
    "        # Calcolo della media annuale (DD)\n",
    "        \n",
    "        annual_mean_dd = combined_data_DD.mean(dim='time', skipna=True)\n",
    "        annual_std_dd = combined_data_DD.std(dim='time', skipna=True)\n",
    "        \n",
    "        annual_means_by_type_DD[t_key] = annual_mean_dd\n",
    "        annual_std_by_type_DD[t_key] = annual_std_dd\n",
    "        \n",
    "        print(f\"DD - Annual mean and std calculation for {t_key} completed.\")\n",
    "\n",
    "        # Calcolo della media mensile (DD)\n",
    "        \n",
    "        monthly_mean_dd = combined_data_DD.groupby('time.month').mean(dim='time', skipna=True)\n",
    "        monthly_std_dd = combined_data_DD.groupby('time.month').std(dim='time', skipna=True)\n",
    "        \n",
    "        monthly_means_by_type_DD[t_key] = monthly_mean_dd\n",
    "        monthly_std_by_type_DD[t_key] = monthly_std_dd\n",
    "        \n",
    "        print(f\"DD - Monthly mean and std calculation for {t_key} completed.\")\n",
    "    else:\n",
    "        print(f\"DD - Nessun dato trovato per {t_key}.\")\n",
    "\n",
    "###### DryDep_Hg2\n",
    "\n",
    "for t_key, data_arrays_list_DD_Hg2 in data_arrays_by_type_DD_Hg2.items():\n",
    "    \n",
    "    if data_arrays_list_DD_Hg2:\n",
    "        \n",
    "        combined_data_DD_Hg2 = xr.concat(data_arrays_list_DD_Hg2, dim='time')\n",
    "        \n",
    "        # Calcolo della media annuale (DD_Hg2)\n",
    "        \n",
    "        annual_mean_dd_Hg2 = combined_data_DD_Hg2.mean(dim='time', skipna=True)\n",
    "        annual_std_dd_Hg2 = combined_data_DD_Hg2.std(dim='time', skipna=True)\n",
    "        \n",
    "        annual_means_by_type_DD_Hg2[t_key] = annual_mean_dd_Hg2\n",
    "        annual_std_by_type_DD_Hg2[t_key] = annual_std_dd_Hg2\n",
    "        \n",
    "        print(f\"DD_Hg2 - Annual mean and std calculation for {t_key} completed.\")\n",
    "\n",
    "        # Calcolo della media mensile (DD_HG2)\n",
    "        \n",
    "        monthly_mean_dd_Hg2 = combined_data_DD_Hg2.groupby('time.month').mean(dim='time', skipna=True)\n",
    "        monthly_std_dd_Hg2 = combined_data_DD_Hg2.groupby('time.month').std(dim='time', skipna=True)\n",
    "        \n",
    "        monthly_means_by_type_DD_Hg2[t_key] = monthly_mean_dd_Hg2\n",
    "        monthly_std_by_type_DD_Hg2[t_key] = monthly_std_dd_Hg2\n",
    "        \n",
    "        print(f\"DD_Hg2 - Monthly mean and std calculation for {t_key} completed.\")\n",
    "    else:\n",
    "        print(f\"DD_Hg2 - Nessun dato trovato per {t_key}.\")\n",
    "\n",
    "###### DryDep_HgP\n",
    "\n",
    "for t_key, data_arrays_list_DD_HgP in data_arrays_by_type_DD_HgP.items():\n",
    "    \n",
    "    if data_arrays_list_DD_HgP:\n",
    "        \n",
    "        combined_data_DD_HgP = xr.concat(data_arrays_list_DD_HgP, dim='time')\n",
    "        \n",
    "        # Calcolo della media annuale (DD_HgP)\n",
    "        \n",
    "        annual_mean_dd_HgP = combined_data_DD_HgP.mean(dim='time', skipna=True)\n",
    "        annual_std_dd_HgP = combined_data_DD_HgP.std(dim='time', skipna=True)\n",
    "        \n",
    "        annual_means_by_type_DD_HgP[t_key] = annual_mean_dd_HgP\n",
    "        annual_std_by_type_DD_HgP[t_key] = annual_std_dd_HgP\n",
    "        \n",
    "        print(f\"DD_HgP - Annual mean and std calculation for {t_key} completed.\")\n",
    "\n",
    "        # Calcolo della media mensile (DD_HgP)\n",
    "        \n",
    "        monthly_mean_dd_HgP = combined_data_DD_HgP.groupby('time.month').mean(dim='time', skipna=True)\n",
    "        monthly_std_dd_HgP = combined_data_DD_HgP.groupby('time.month').std(dim='time', skipna=True)\n",
    "        \n",
    "        monthly_means_by_type_DD_HgP[t_key] = monthly_mean_dd_HgP\n",
    "        monthly_std_by_type_DD_HgP[t_key] = monthly_std_dd_HgP\n",
    "        \n",
    "        print(f\"DD_Hg2 - Monthly mean and std calculation for {t_key} completed.\")\n",
    "    else:\n",
    "        print(f\"DD_Hg2 - Nessun dato trovato per {t_key}.\")\n",
    "\n",
    "###### DryDep_Hg0\n",
    "\n",
    "for t_key, data_arrays_list_DD_Hg0 in data_arrays_by_type_DD_Hg0.items():\n",
    "    \n",
    "    if data_arrays_list_DD_Hg0:\n",
    "        \n",
    "        combined_data_DD_Hg0 = xr.concat(data_arrays_list_DD_Hg0, dim='time')\n",
    "        \n",
    "        # Calcolo della media annuale (DD_Hg0)\n",
    "        \n",
    "        annual_mean_dd_Hg0 = combined_data_DD_Hg0.mean(dim='time', skipna=True)\n",
    "        annual_std_dd_Hg0 = combined_data_DD_Hg0.std(dim='time', skipna=True)\n",
    "        \n",
    "        annual_means_by_type_DD_Hg0[t_key] = annual_mean_dd_Hg0\n",
    "        annual_std_by_type_DD_Hg0[t_key] = annual_std_dd_Hg0\n",
    "        \n",
    "        print(f\"DD_Hg0 - Annual mean and std calculation for {t_key} completed.\")\n",
    "\n",
    "        # Calcolo della media mensile (DD_Hg0)\n",
    "        \n",
    "        monthly_mean_dd_Hg0 = combined_data_DD_Hg0.groupby('time.month').mean(dim='time', skipna=True)\n",
    "        monthly_std_dd_Hg0 = combined_data_DD_Hg0.groupby('time.month').std(dim='time', skipna=True)\n",
    "        \n",
    "        monthly_means_by_type_DD_Hg0[t_key] = monthly_mean_dd_Hg0\n",
    "        monthly_std_by_type_DD_Hg0[t_key] = monthly_std_dd_Hg0\n",
    "        \n",
    "        print(f\"DD_Hg0 - Monthly mean and std calculation for {t_key} completed.\")\n",
    "    else:\n",
    "        print(f\"DD_Hg0 - Nessun dato trovato per {t_key}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddbc684-1f63-419e-9aca-893434f845ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary to store the converted data\n",
    "annual_means_by_type_DD_micro = {}\n",
    "monthly_means_by_type_DD_micro = {}\n",
    "annual_std_by_type_DD_micro = {}\n",
    "monthly_std_by_type_DD_micro = {}\n",
    "\n",
    "annual_means_by_type_DD_micro_Hg2 = {}\n",
    "monthly_means_by_type_DD_micro_Hg2 = {}\n",
    "annual_std_by_type_DD_micro_Hg2 = {}\n",
    "monthly_std_by_type_DD_micro_Hg2 = {}\n",
    "\n",
    "annual_means_by_type_DD_micro_HgP = {}\n",
    "monthly_means_by_type_DD_micro_HgP = {}\n",
    "annual_std_by_type_DD_micro_HgP = {}\n",
    "monthly_std_by_type_DD_micro_HgP = {}\n",
    "\n",
    "annual_means_by_type_DD_micro_Hg0 = {}\n",
    "monthly_means_by_type_DD_micro_Hg0 = {}\n",
    "annual_std_by_type_DD_micro_Hg0 = {}\n",
    "monthly_std_by_type_DD_micro_Hg0 = {}\n",
    "\n",
    "# Iterate through the dictionary and multiply each DataArray by the conversion factor\n",
    "\n",
    "for t_key, data_array in annual_means_by_type_DD.items():\n",
    "    \n",
    "    annual_means_by_type_DD_micro[t_key] = data_array * cf_units_dd_year\n",
    "\n",
    "for t_key, data_array in monthly_means_by_type_DD.items():\n",
    "    \n",
    "    monthly_means_by_type_DD_micro[t_key] = data_array * cf_units_dd_year\n",
    "    \n",
    "for t_key, data_array in annual_std_by_type_DD.items():\n",
    "    \n",
    "    annual_std_by_type_DD_micro[t_key] = data_array * cf_units_dd_year\n",
    "    \n",
    "for t_key, data_array in monthly_std_by_type_DD.items():\n",
    "    \n",
    "    monthly_std_by_type_DD_micro[t_key] = data_array * cf_units_dd_year\n",
    "\n",
    "print(\"DD TOTAL - Unit conversion completed.\")\n",
    "\n",
    "# Iterate through the dictionary and multiply each DataArray by the conversion factor\n",
    "\n",
    "for t_key, data_array in annual_means_by_type_DD_Hg2.items():\n",
    "    \n",
    "    annual_means_by_type_DD_micro_Hg2[t_key] = data_array * cf_units_dd_year\n",
    "\n",
    "for t_key, data_array in monthly_means_by_type_DD_Hg2.items():\n",
    "    \n",
    "    monthly_means_by_type_DD_micro_Hg2[t_key] = data_array * cf_units_dd_year\n",
    "    \n",
    "for t_key, data_array in annual_std_by_type_DD_Hg2.items():\n",
    "    \n",
    "    annual_std_by_type_DD_micro_Hg2[t_key] = data_array * cf_units_dd_year\n",
    "    \n",
    "for t_key, data_array in monthly_std_by_type_DD_Hg2.items():\n",
    "    \n",
    "    monthly_std_by_type_DD_micro_Hg2[t_key] = data_array * cf_units_dd_year\n",
    "\n",
    "print(\"DD Hg2 - Unit conversion completed.\")\n",
    "\n",
    "# Iterate through the dictionary and multiply each DataArray by the conversion factor\n",
    "\n",
    "for t_key, data_array in annual_means_by_type_DD_HgP.items():\n",
    "    \n",
    "    annual_means_by_type_DD_micro_HgP[t_key] = data_array * cf_units_dd_year\n",
    "\n",
    "for t_key, data_array in monthly_means_by_type_DD_HgP.items():\n",
    "    \n",
    "    monthly_means_by_type_DD_micro_HgP[t_key] = data_array * cf_units_dd_year\n",
    "    \n",
    "for t_key, data_array in annual_std_by_type_DD_HgP.items():\n",
    "    \n",
    "    annual_std_by_type_DD_micro_HgP[t_key] = data_array * cf_units_dd_year\n",
    "    \n",
    "for t_key, data_array in monthly_std_by_type_DD_HgP.items():\n",
    "    \n",
    "    monthly_std_by_type_DD_micro_HgP[t_key] = data_array * cf_units_dd_year\n",
    "\n",
    "print(\"DD HgP - Unit conversion completed.\")\n",
    "\n",
    "# Iterate through the dictionary and multiply each DataArray by the conversion factor\n",
    "\n",
    "for t_key, data_array in annual_means_by_type_DD_Hg0.items():\n",
    "    \n",
    "    annual_means_by_type_DD_micro_Hg0[t_key] = data_array * cf_units_dd_year\n",
    "\n",
    "for t_key, data_array in monthly_means_by_type_DD_Hg0.items():\n",
    "    \n",
    "    monthly_means_by_type_DD_micro_Hg0[t_key] = data_array * cf_units_dd_year\n",
    "    \n",
    "for t_key, data_array in annual_std_by_type_DD_Hg0.items():\n",
    "    \n",
    "    annual_std_by_type_DD_micro_Hg0[t_key] = data_array * cf_units_dd_year\n",
    "    \n",
    "for t_key, data_array in monthly_std_by_type_DD_Hg0.items():\n",
    "    \n",
    "    monthly_std_by_type_DD_micro_Hg0[t_key] = data_array * cf_units_dd_year\n",
    "\n",
    "print(\"DD Hg0 - Unit conversion completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e5327-9da2-47a7-842e-35f331c71ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(monthly_std_by_type_DD_micro_Hg0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1da09-e107-4aab-bede-4477fa6853bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- 1. Nuove Variabili di Selezione ---\n",
    "# Coordinate dell'Isola Macquarie\n",
    "target_lat = -54.5\n",
    "target_lon = 158.95\n",
    "\n",
    "# La chiave specifica che hai scelto\n",
    "selected_key = 'NP' \n",
    "\n",
    "# Una lista dei tuoi dizionari (come definiti in precedenza)\n",
    "list_of_data_dicts = [\n",
    "    monthly_means_by_type_DD_micro_Hg0,\n",
    "    monthly_means_by_type_DD_micro_Hg2,\n",
    "    monthly_means_by_type_DD_micro_HgP,\n",
    "    #monthly_means_by_type_DD_micro\n",
    "]\n",
    "\n",
    "# Etichette per le serie nel grafico\n",
    "series_labels = ['DD_Hg0', 'DD_Hg2', 'DD_HgP']#, 'DD_Total']\n",
    "\n",
    "# Dizionario per contenere le serie temporali finali (month-only)\n",
    "monthly_data_series = {}\n",
    "\n",
    "# --- 2. Processo di Estrazione al Punto Specifico ---\n",
    "for data_dict, label in zip(list_of_data_dicts, series_labels):\n",
    "    if selected_key in data_dict:\n",
    "        da = data_dict[selected_key]\n",
    "        \n",
    "        # **NUOVO PASSO CRUCIALE:** # Seleziona il punto sulla mappa più vicino alle coordinate fornite.\n",
    "        # Questo riduce la DataArray da (month, lat, lon) a (month).\n",
    "        point_series = da.sel(\n",
    "            lat=target_lat, \n",
    "            lon=target_lon, \n",
    "            method='nearest' # Trova il punto della griglia più vicino\n",
    "        )\n",
    "        \n",
    "        # 3. Converti la DataArray risultante in pandas.Series e memorizza\n",
    "        # .to_series() è necessario per inserire i dati nel DataFrame di pandas\n",
    "        monthly_data_series[label] = point_series.to_series()\n",
    "    else:\n",
    "        print(f\"Attenzione: La chiave '{selected_key}' non è stata trovata nel dizionario per {label}\")\n",
    "\n",
    "# 4. Combina tutte le serie in un unico DataFrame\n",
    "df_plot = pd.DataFrame(monthly_data_series)\n",
    "\n",
    "# Opzionale: Rinomina l'indice 'month' per un'etichetta migliore nel plot\n",
    "df_plot.index.name = \"Mese\"\n",
    "\n",
    "\n",
    "# 5. Crea il Grafico ad Area Stacked\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "ax = df_plot.plot.area(\n",
    "    stacked=True, # Area Plot Stacked (somma delle aree)\n",
    "    ax=plt.gca(),\n",
    "    title=f\"Monthly Hg Dry Deposition (2018-2022 period) @ MQI - Perturbation key: {selected_key} (Lat: {target_lat}, Lon: {target_lon})\"\n",
    ")\n",
    "\n",
    "# 6. Personalizzazione e Visualizzazione\n",
    "ax.set_xlabel(\"Month\")\n",
    "ax.set_ylabel(\"Hg dry deposition (ug * m-2 * year-1)\")\n",
    "ax.legend(title=\"Perturbation keys\", loc='upper right')\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Imposta etichette sull'asse X per i 12 mesi\n",
    "\n",
    "ax.set_xticks(range(1, 13))\n",
    "ax.set_xticklabels(month_names)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d482a5-7334-4edd-81fb-a237a8bf375a",
   "metadata": {},
   "source": [
    "Ricorda che in questo plot il totale e' sbagliato, non deve metterlo sopra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c69410f-2969-4491-9033-461e8bcee7c6",
   "metadata": {},
   "source": [
    "## Hg2 to SSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fefea1-43ad-4442-a613-b8788f90cad5",
   "metadata": {},
   "source": [
    "Ok, let's see how much Hg2 is lost on to the SSA. La variabile che cerco e' Hg2GasToSSA in MercuryChem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a991f7-195d-4566-97aa-2260bd1acd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_types = ['HgChem','Met']\n",
    "\n",
    "SSAloss_dict_x = {}\n",
    "\n",
    "for y in years:\n",
    "    \n",
    "    for t in perturbation_types:\n",
    "            \n",
    "            # Build the key dynamically\n",
    "           \n",
    "            path_key_Hg2toSSA= f\"{perturbation_type_map[t]}_{y}_{data_types[0]}\"\n",
    "            # print(path_key_Hg2toSSA)\n",
    "            path_key_Met= f\"{perturbation_type_map[t]}_{y}_{data_types[1]}\"\n",
    "            # print(path_key_Met)\n",
    "        \n",
    "            # Hg2GasToSSA is in molecules/cm3*s, must be converted in ug/m2*year\n",
    "        \n",
    "            dataset_Hg2toSSA = datasets_by_year[path_key_Hg2toSSA]['Hg2GasToSSA']\n",
    "            dataset_met1 = datasets_by_year[path_key_Met]['Met_AIRVOL']\n",
    "            dataset_met2 = datasets_by_year[path_key_Met]['AREA']\n",
    "            dataset = (( dataset_met1 * dataset_Hg2toSSA * conv ) / dataset_met2) # << Met_AIRVOL*AREA*Hg2GasToSSA*conv\n",
    "\n",
    "            result_key = f\"{perturbation_type_map[t]}_{y}\"\n",
    "            # Save the dataset in the main dictionary only if it's valid\n",
    "            if dataset is not None:\n",
    "                SSAloss_dict_x[result_key] = dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b501972-b04c-47b2-8639-94479d28f9a7",
   "metadata": {},
   "source": [
    "Quindi, mi serve la media mensile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c0e24f-3753-4002-a6d8-ac4917e3dddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the dataset for a specific run\n",
    "sample_dataset = SSAloss_dict_x['NP_2018']\n",
    "\n",
    "# Then, you would typically look at its dimensions/coordinates\n",
    "# If it's an xarray object:\n",
    "print(sample_dataset.coords)\n",
    "print(sample_dataset.dims)\n",
    "print(sample_dataset['lat'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19417736-7504-45fc-9acf-b8f1fa13c31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd # pandas is often imported for xarray operations\n",
    "\n",
    "# --- Assumed Input Variables ---\n",
    "# SSAloss_dict_x: Your dictionary containing DataArrays, e.g., {'NP_2018': <DataArray>, 'OL_2018': <DataArray>, ...}\n",
    "# perturbation_types: The unique prefixes (scenarios) deduced from your keys.\n",
    "perturbation_types = ['NP', 'OL', 'OM', 'SSAC', 'SSA0', 'WL', 'WM']\n",
    "# -------------------------------\n",
    "\n",
    "# Dictionary to store the final 5-year averages for each scenario\n",
    "SSAloss_5year_avg = {}\n",
    "\n",
    "for p_type in perturbation_types:\n",
    "    # 1. Gather keys for the current perturbation type across all years\n",
    "    keys_for_p_type = [k for k in SSAloss_dict_x.keys() if k.startswith(p_type)]\n",
    "    \n",
    "    # 2. Extract DataArrays and Concatenate along the 'time' dimension\n",
    "    datasets_to_combine = [SSAloss_dict_x[k] for k in keys_for_p_type]\n",
    "    \n",
    "    # This creates a single DataArray (e.g., 60 months for 5 years)\n",
    "    combined_dataset = xr.concat(datasets_to_combine, dim='time')\n",
    "    \n",
    "    # 3. Calculate Time-Weighted Average (Correct for month length)\n",
    "\n",
    "    # 3a. Get the length of each month (e.g., 31, 28, 31, ...)\n",
    "    # xarray's .dt accessor pulls this from the 'time' coordinate\n",
    "    month_length = combined_dataset.time.dt.days_in_month\n",
    "\n",
    "    # 3b. Normalize the month lengths to create weights that sum to 1\n",
    "    # We use .where() to avoid weights for any months that might be missing (NaN)\n",
    "    weights = month_length.where(~combined_dataset.isnull())\n",
    "    weights /= weights.sum(dim='time')\n",
    "    \n",
    "    # 3c. Apply the weights and sum along the 'time' dimension\n",
    "    # (Data * Weights) summed is mathematically equivalent to the weighted mean.\n",
    "    five_year_avg = (combined_dataset * weights).sum(dim='time')\n",
    "    \n",
    "    # 4. Store the result with the perturbation type as the key\n",
    "    SSAloss_5year_avg[p_type] = five_year_avg\n",
    "\n",
    "print(\"Calculation Complete. SSAloss_5year_avg now holds the 5-year weighted average for each scenario.\")\n",
    "# Example of how to inspect a result:\n",
    "# print(SSAloss_5year_avg['NP'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72efea95-586c-48ca-8fa5-d3b638844a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- ASSUMED INPUT VARIABLES ---\n",
    "# SSAloss_dict_x: Your dictionary containing DataArrays, e.g., \n",
    "# {'NP_2018': <DataArray>, 'OL_2018': <DataArray>, ...}\n",
    "# Each DataArray is assumed to have a 'time' dimension.\n",
    "# -------------------------------\n",
    "\n",
    "# The unique prefixes (scenarios)\n",
    "perturbation_types = ['NP', 'OL', 'OM', 'SSAC', 'SSA0', 'WL', 'WM']\n",
    "\n",
    "# Dictionary to store the final 12-month climatology for each scenario\n",
    "SSAloss_monthly_climatology = {}\n",
    "\n",
    "print(\"Starting calculation of 5-year monthly climatology...\")\n",
    "\n",
    "for p_type in perturbation_types:\n",
    "    # 1. Gather keys for the current perturbation type across all years\n",
    "    keys_for_p_type = [k for k in SSAloss_dict_x.keys() if k.startswith(p_type)]\n",
    "    \n",
    "    # 2. Extract DataArrays and Concatenate along the 'time' dimension\n",
    "    datasets_to_combine = [SSAloss_dict_x[k] for k in keys_for_p_type]\n",
    "    \n",
    "    # This creates a single, long DataArray (e.g., 60 months for 5 years)\n",
    "    combined_dataset = xr.concat(datasets_to_combine, dim='time')\n",
    "    \n",
    "    # 3. Calculate the Climatological Monthly Average\n",
    "    \n",
    "    # We use .groupby('time.month') to group all 60 time points into 12 groups (Jan, Feb, ..., Dec).\n",
    "    # Then we use .mean() to average the data within those 5 instances of each month.\n",
    "    # The result has a new dimension called 'month' (size 12).\n",
    "    monthly_climatology = combined_dataset.groupby('time.month').mean(dim='time')\n",
    "    \n",
    "    # 4. Store the result\n",
    "    SSAloss_monthly_climatology[p_type] = monthly_climatology\n",
    "    print(f\"Climatology calculated for scenario: {p_type}\")\n",
    "\n",
    "print(\"\\nCalculation Complete.\")\n",
    "print(\"SSAloss_monthly_climatology now holds the 7 DataArrays needed for time series plotting.\")\n",
    "# Example of how to inspect the dimensions of a result:\n",
    "# print(SSAloss_monthly_climatology['NP'])\n",
    "\n",
    "# --- MOCKUP FOR DEMONSTRATION PURPOSES ---\n",
    "# If your SSAloss_dict_x is not defined, this block runs a small example\n",
    "if 'SSAloss_dict_x' not in globals():\n",
    "    print(\"\\n--- Running Mockup Example ---\")\n",
    "    \n",
    "    # Create mock time coordinates for 5 years (60 months)\n",
    "    time_coords = pd.date_range('2018-01-15', periods=60, freq='M')\n",
    "    \n",
    "    # Mock spatial coordinates\n",
    "    mock_lev = np.array([1000, 500])\n",
    "    mock_lat = np.array([45, 55])\n",
    "    mock_lon = np.array([0, 10])\n",
    "    \n",
    "    # Create a simple, synthetic SSA loss value that varies seasonally\n",
    "    seasonal_data = np.sin(np.linspace(0, 2*np.pi * 5, 60))[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    mock_data = (seasonal_data + 1) * 1e-4\n",
    "    \n",
    "    # Create mock dictionary structure\n",
    "    mock_dict = {}\n",
    "    for year in range(2018, 2023):\n",
    "        start_idx = (year - 2018) * 12\n",
    "        end_idx = start_idx + 12\n",
    "        \n",
    "        # Mock DataArray for a single year (e.g., 'NP_2018')\n",
    "        mock_da = xr.DataArray(\n",
    "            mock_data[start_idx:end_idx, :, :, :].squeeze(), # squeeze removes the unnecessary dimension\n",
    "            coords={\n",
    "                'time': time_coords[start_idx:end_idx],\n",
    "                'lev': mock_lev, \n",
    "                'lat': mock_lat, \n",
    "                'lon': mock_lon\n",
    "            },\n",
    "            dims=['time', 'lev', 'lat', 'lon'],\n",
    "            name='SSAloss'\n",
    "        )\n",
    "        mock_dict[f'NP_{year}'] = mock_da\n",
    "    \n",
    "    # Run the calculation with mock data\n",
    "    SSAloss_dict_x = mock_dict\n",
    "    \n",
    "    # Re-run the core logic to calculate climatology on the mock data\n",
    "    mock_climatology = {}\n",
    "    \n",
    "    # Simplified loop for mock example\n",
    "    datasets_to_combine = [SSAloss_dict_x[k] for k in SSAloss_dict_x.keys()]\n",
    "    combined_dataset = xr.concat(datasets_to_combine, dim='time')\n",
    "    mock_climatology['NP'] = combined_dataset.groupby('time.month').mean(dim='time')\n",
    "    \n",
    "    print(\"\\n--- Mock Climatology Result (NP) ---\")\n",
    "    print(mock_climatology['NP'])\n",
    "    print(\"Dimensions:\", mock_climatology['NP'].dims) # Should show ('month', 'lev', 'lat', 'lon')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4121dd-4e79-4dc0-bf65-d41fe3e1af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the dataset for a specific run\n",
    "sample_dataset = SSAloss_5year_avg['NP']\n",
    "\n",
    "# Then, you would typically look at its dimensions/coordinates\n",
    "# If it's an xarray object:\n",
    "print(sample_dataset.coords)\n",
    "print(sample_dataset.dims)\n",
    "print(sample_dataset['lat'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba31d4b-cec7-40bc-afa0-ffc700d0363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "\n",
    "# --- NOTE TO USER: SETUP ASSUMPTION ---\n",
    "# This code assumes you have already defined the 'SSAloss_5year_avg' \n",
    "# dictionary in your Python session, containing the 5-year averages \n",
    "# for each scenario, including 'NP'.\n",
    "# -------------------------------------\n",
    "\n",
    "def plot_ssa_loss_map(ssa_loss_dict, case_name='NP', dim_to_sum='lev', title_suffix=\"5-Year Average (2018-2022)\"):\n",
    "    \"\"\"\n",
    "    Generates a global map for a specified case, calculated as the SUM (Total) \n",
    "    across the vertical dimension.\n",
    "    \n",
    "    Args:\n",
    "        ssa_loss_dict (dict): Dictionary containing 3D xarray DataArrays (lev, lat, lon).\n",
    "        case_name (str): The key in the dictionary to plot (e.g., 'NP').\n",
    "        dim_to_sum (str): The dimension to sum over to get a 2D map (e.g., 'lev').\n",
    "        title_suffix (str): Text to append to the plot title.\n",
    "    \"\"\"\n",
    "    \n",
    "    if case_name not in ssa_loss_dict:\n",
    "        print(f\"Error: Case '{case_name}' not found in the dictionary.\")\n",
    "        return\n",
    "\n",
    "    # 1. Select the 3D data (lev, lat, lon)\n",
    "    data_3d = ssa_loss_dict[case_name]\n",
    "    \n",
    "    # 2. Calculate the SUM across the vertical dimension ('lev') to get a 2D map (lat, lon)\n",
    "    # The result represents the TOTAL integrated SSA loss throughout the entire atmospheric column.\n",
    "    data_2d = data_3d.sum(dim=dim_to_sum) # CHANGED FROM .mean() TO .sum()\n",
    "\n",
    "    # Determine plot properties\n",
    "    plot_title = f\"{case_name} Sea Salt Uptake - Column Total (Summed Over Level) {title_suffix}\"\n",
    "    \n",
    "    # Set up the figure and the GeoAxes\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Define the projection for the map display\n",
    "    # PlateCarree is a simple, rectangular global map projection\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=ccrs.PlateCarree())\n",
    "    \n",
    "    # Add map features for context\n",
    "    ax.coastlines(resolution='50m', color='gray')\n",
    "    ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    \n",
    "    # 3. Plot the data using xarray's built-in plotting method (pcolormesh)\n",
    "    # The 'transform' argument tells cartopy what projection the data coordinates are in.\n",
    "    # The data is already in standard lat/lon, so we use ccrs.PlateCarree().\n",
    "    plot = data_2d.plot.pcolormesh(\n",
    "        ax=ax,\n",
    "        transform=ccrs.PlateCarree(),\n",
    "        cmap='viridis',  # You can choose any matplotlib colormap\n",
    "        cbar_kwargs={\n",
    "            'label': 'ug/m2*year', # Updated label\n",
    "            'shrink': 0.8\n",
    "        },\n",
    "        extend='both', # Extend the colorbar for values outside the normal range\n",
    "        # You might need to adjust vmin/vmax based on your actual data range\n",
    "        # vmin=1e-5,\n",
    "        # vmax=1e-3, \n",
    "        \n",
    "    )\n",
    "\n",
    "    ax.set_title(plot_title)\n",
    "    \n",
    "    # Optional: Set the extent of the map (full globe)\n",
    "    ax.set_global() \n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# --- Placeholder for the execution call ---\n",
    "# You need to run this function with your actual dictionary.\n",
    "# Since I cannot access your environment, I will show the call here.\n",
    "# Assuming SSAloss_5year_avg exists:\n",
    "# plot_ssa_loss_map(SSAloss_5year_avg, case_name='NP')\n",
    "\n",
    "# --- MOCKUP DATA FOR RUNNABILITY (Replace with your actual execution) ---\n",
    "# Since this code must be runnable, I will generate mock data that matches your structure\n",
    "# If you run this file directly, it will use the mock data.\n",
    "if 'SSAloss_5year_avg' not in globals():\n",
    "    print(\"\\n--- Generating MOCK Data for Demo ---\")\n",
    "    \n",
    "    # Create mock coordinates\n",
    "    mock_lev = np.linspace(0, 1, 10)\n",
    "    mock_lat = np.arange(-89, 90, 4)\n",
    "    mock_lon = np.arange(-180, 180, 5)\n",
    "    \n",
    "    # Create mock data array (3D: lev, lat, lon)\n",
    "    # This mock data is designed to show a higher loss in the Arctic (high lat)\n",
    "    lon_grid, lat_grid = np.meshgrid(mock_lon, mock_lat)\n",
    "    mock_values = (1 + np.sin(np.deg2rad(lat_grid) * 3)) * np.cos(np.deg2rad(lon_grid) / 5)\n",
    "    \n",
    "    # Add a level dimension to the mock data and scale it\n",
    "    mock_data = mock_values[np.newaxis, :, :] * np.exp(-mock_lev[:, np.newaxis, np.newaxis] * 2) * 1e-4\n",
    "    \n",
    "    # Create the mock DataArray\n",
    "    mock_da = xr.DataArray(\n",
    "        mock_data,\n",
    "        coords={'lev': mock_lev, 'lat': mock_lat, 'lon': mock_lon},\n",
    "        dims=['lev', 'lat', 'lon'],\n",
    "        name='SSAloss'\n",
    "    )\n",
    "    \n",
    "    # Create the mock SSAloss_5year_avg dictionary\n",
    "    mock_SSAloss_5year_avg = {'NP': mock_da}\n",
    "    \n",
    "    # Execute the plotting function with the mock data\n",
    "    plot_ssa_loss_map(mock_SSAloss_5year_avg, case_name='NP')\n",
    "else:\n",
    "    # Execute the plotting function with the real data\n",
    "    plot_ssa_loss_map(SSAloss_5year_avg, case_name='SSA0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f4dcd-18c6-49e3-99d1-03fdfe365ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "# --- NOTE TO USER: SETUP ASSUMPTION ---\n",
    "# This code assumes the 'SSAloss_monthly_climatology' dictionary has \n",
    "# already been created using the 'calculate_climatology.py' script \n",
    "# (4D DataArrays: month, lev, lat, lon).\n",
    "# -------------------------------------\n",
    "\n",
    "def plot_climatological_timeseries(climatology_dict, case_name='NP', plot_title=\"Climatological SSA Loss Cycle (Global Mean, Column Summed)\"):\n",
    "    \"\"\"\n",
    "    Plots the 12-month climatological cycle for a specified scenario.\n",
    "    \n",
    "    The data is spatially averaged (lat/lon) and summed vertically (lev) \n",
    "    to represent the total global atmospheric impact over the year.\n",
    "    \n",
    "    Args:\n",
    "        climatology_dict (dict): Dictionary containing the 4D monthly climatology DataArrays.\n",
    "        case_name (str): The key in the dictionary to plot (e.g., 'NP').\n",
    "        plot_title (str): The title for the resulting plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    if case_name not in climatology_dict:\n",
    "        print(f\"Error: Case '{case_name}' not found in the climatology dictionary.\")\n",
    "        return\n",
    "\n",
    "    # 1. Select the 4D climatology data (month, lev, lat, lon)\n",
    "    data_4d = climatology_dict[case_name]\n",
    "    \n",
    "    # 2. Reduce the data to a 1D time series (size 12)\n",
    "    # a. Sum across the vertical dimension ('lev') to get the Column Total.\n",
    "    data_column_sum = data_4d.sum(dim='lev')\n",
    "    \n",
    "    # b. Calculate the Global Mean (mean across 'lat' and 'lon').\n",
    "    # The final result is a 1D array indexed by 'month'.\n",
    "    time_series_data = data_column_sum.mean(dim=['lat', 'lon'])\n",
    "    \n",
    "    # 3. Prepare plot coordinates\n",
    "    months = time_series_data['month'].values\n",
    "    month_names = [calendar.month_abbr[m] for m in months]\n",
    "    \n",
    "    # 4. Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Plot the time series\n",
    "    ax.plot(months, time_series_data.values, marker='o', linestyle='-', color='indigo', linewidth=2)\n",
    "    \n",
    "    # Set X-axis to display month names\n",
    "    ax.set_xticks(months)\n",
    "    ax.set_xticklabels(month_names)\n",
    "    \n",
    "    # Labeling and Titling\n",
    "    ax.set_title(f\"{case_name} {plot_title}\", fontsize=14)\n",
    "    ax.set_xlabel(\"Month of the Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Total Global SSA Loss (Column Summed)\", fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Improve aesthetics\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- MOCKUP DATA FOR RUNNABILITY (Replace with your actual execution) ---\n",
    "# If your SSAloss_monthly_climatology is not defined, this block runs a small example\n",
    "if 'SSAloss_monthly_climatology' not in globals():\n",
    "    print(\"\\n--- Running Mockup Example for Plotting ---\")\n",
    "    \n",
    "    # Create mock 4D climatology data (12 months, 2 lev, 2 lat, 2 lon)\n",
    "    mock_months = np.arange(1, 13)\n",
    "    mock_lev = np.array([1000, 500])\n",
    "    mock_lat = np.array([45, 55])\n",
    "    mock_lon = np.array([0, 10])\n",
    "    \n",
    "    # Create synthetic seasonal data (peaks in summer, dips in winter)\n",
    "    seasonal_data = (np.cos(np.linspace(0, 2 * np.pi, 12) + np.pi/2) * 0.5 + 1.5) * 1e-4\n",
    "    \n",
    "    # Expand data to 4D structure\n",
    "    mock_4d_data = seasonal_data[:, np.newaxis, np.newaxis, np.newaxis] * np.ones((12, 2, 2, 2))\n",
    "    \n",
    "    mock_da = xr.DataArray(\n",
    "        mock_4d_data,\n",
    "        coords={'month': mock_months, 'lev': mock_lev, 'lat': mock_lat, 'lon': mock_lon},\n",
    "        dims=['month', 'lev', 'lat', 'lon'],\n",
    "        name='SSAloss'\n",
    "    )\n",
    "    \n",
    "    # Create the mock climatology dictionary\n",
    "    mock_climatology = {'NP': mock_da}\n",
    "    \n",
    "    # Execute the plotting function with the mock data\n",
    "    plot_climatological_timeseries(mock_climatology, case_name='NP')\n",
    "\n",
    "else:\n",
    "    # Execute the plotting function with the real data\n",
    "    plot_climatological_timeseries(SSAloss_monthly_climatology, case_name='OL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc59eb-1eda-47f8-bb29-434b6ae52f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "# --- NOTE TO USER: SETUP ASSUMPTION ---\n",
    "# This code assumes the 'SSAloss_monthly_climatology' dictionary has \n",
    "# already been created using the 'calculate_climatology.py' script \n",
    "# (4D DataArrays: month, lev, lat, lon).\n",
    "# -------------------------------------\n",
    "\n",
    "# Define all scenarios\n",
    "ALL_PERTURBATION_TYPES = ['NP', 'OL', 'OM', 'SSAC', 'SSA0', 'WL', 'WM']\n",
    "\n",
    "# Defining maps' colors, line style, and legend labels (as requested)\n",
    "colors = {\n",
    "    'NP': 'black', 'WM': 'blue', 'WL': 'lightblue',\n",
    "    'OM': 'green', 'OL': 'lightgreen', 'SSA0': 'red', 'SSAC': 'yellow'\n",
    "}\n",
    "\n",
    "linestyles = {\n",
    "    'NP': '--', 'WM': '-', 'WL': '-',\n",
    "    'OM': '-', 'OL': '-', 'SSA0': '-', 'SSAC': '-'\n",
    "}\n",
    "\n",
    "legend_map = {\n",
    "    'NP': 'No Perturbations',\n",
    "    'WM': 'More_Wind',\n",
    "    'WL': 'Less_Wind',\n",
    "    'OM': 'More_Ocean',\n",
    "    'OL': 'Less_Ocean',\n",
    "    'SSA0': 'Sea Salt Aerosols (Zero)',\n",
    "    'SSAC': 'Sea Salt Aerosols (Constant)'\n",
    "}\n",
    "\n",
    "# 1-character month abbreviations for X-axis labels (as requested)\n",
    "MONTH_ABBREVIATIONS = ['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D']\n",
    "\n",
    "\n",
    "def plot_climatological_timeseries_comparison(climatology_dict, plot_title=\"Sea Salt Uptake - Comparison (Global Mean (2018-2022), Column Summed)\"):\n",
    "    \"\"\"\n",
    "    Plots the 12-month climatological cycle for ALL specified scenarios \n",
    "    on a single graph for comparison, using custom colors and linestyles.\n",
    "    \n",
    "    The data is spatially averaged (lat/lon) and summed vertically (lev) \n",
    "    to represent the total global atmospheric impact over the year.\n",
    "    \n",
    "    Args:\n",
    "        climatology_dict (dict): Dictionary containing the 4D monthly climatology DataArrays.\n",
    "        plot_title (str): The title for the resulting plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all(p in climatology_dict for p in ALL_PERTURBATION_TYPES):\n",
    "        missing = [p for p in ALL_PERTURBATION_TYPES if p not in climatology_dict]\n",
    "        print(f\"Error: Missing cases in dictionary: {missing}. Cannot plot all series.\")\n",
    "        return\n",
    "\n",
    "    # 1. Setup the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Get month coordinates (same for all cases)\n",
    "    first_case = ALL_PERTURBATION_TYPES[0]\n",
    "    months = climatology_dict[first_case]['month'].values\n",
    "    \n",
    "    # Use the global 1-character month names as requested\n",
    "    month_names = MONTH_ABBREVIATIONS\n",
    "\n",
    "    # 2. Loop through all perturbation types and plot the series\n",
    "    for p_type in ALL_PERTURBATION_TYPES:\n",
    "        data_4d = climatology_dict[p_type]\n",
    "        \n",
    "        # a. Sum across the vertical dimension ('lev') to get the Column Total.\n",
    "        data_column_sum = data_4d.sum(dim='lev')\n",
    "        \n",
    "        # b. Calculate the Global Mean (mean across 'lat' and 'lon').\n",
    "        time_series_data = data_column_sum.mean(dim=['lat', 'lon'])\n",
    "        \n",
    "        # c. Plot the series using custom colors, linestyles, and legend labels\n",
    "        ax.plot(\n",
    "            months, \n",
    "            time_series_data.values, \n",
    "            marker='o', \n",
    "            linewidth=2,\n",
    "            color=colors.get(p_type, 'gray'),             # Use new colors dictionary\n",
    "            linestyle=linestyles.get(p_type, '-'),        # Use new linestyles dictionary\n",
    "            label=legend_map.get(p_type, p_type)          # Use new legend_map dictionary\n",
    "        )\n",
    "    \n",
    "    # 3. Final Plot Customization\n",
    "    \n",
    "    # Set X-axis to display 1-character month names\n",
    "    ax.set_xticks(months)\n",
    "    ax.set_xticklabels(month_names)\n",
    "    \n",
    "    # Labeling and Titling\n",
    "    ax.set_title(plot_title, fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel(\"Month of the Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Total Global Sea Salt Uptake (ug/m2*year)\", fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Add Legend to distinguish the lines\n",
    "    ax.legend(title=\"Scenario\", frameon=True, shadow=True, loc='best')\n",
    "    \n",
    "    # Improve aesthetics\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- MOCKUP DATA FOR RUNNABILITY (Replace with your actual execution) ---\n",
    "# If your SSAloss_monthly_climatology is not defined, this block runs a small example\n",
    "if 'SSAloss_monthly_climatology' not in globals():\n",
    "    print(\"\\n--- Running Mockup Example for Plotting All Series ---\")\n",
    "    \n",
    "    # Create mock 4D climatology data (12 months, 2 lev, 2 lat, 2 lon)\n",
    "    mock_months = np.arange(1, 13)\n",
    "    mock_lev = np.array([1000, 500])\n",
    "    mock_lat = np.array([45, 55])\n",
    "    mock_lon = np.array([0, 10])\n",
    "    \n",
    "    # Create synthetic seasonal data (peaks in summer, dips in winter)\n",
    "    base_seasonal = (np.cos(np.linspace(0, 2 * np.pi, 12) + np.pi/2) * 0.5 + 1.5)\n",
    "    \n",
    "    # Create the mock climatology dictionary with varied scales\n",
    "    mock_climatology = {}\n",
    "    \n",
    "    for i, p_type in enumerate(ALL_PERTURBATION_TYPES):\n",
    "        # Scale each type slightly differently\n",
    "        scale = 1e-4 * (1 + i * 0.1) \n",
    "        \n",
    "        # Expand data to 4D structure\n",
    "        mock_4d_data = (base_seasonal * scale)[:, np.newaxis, np.newaxis, np.newaxis] * np.ones((12, 2, 2, 2))\n",
    "        \n",
    "        mock_da = xr.DataArray(\n",
    "            mock_4d_data,\n",
    "            coords={'month': mock_months, 'lev': mock_lev, 'lat': mock_lat, 'lon': mock_lon},\n",
    "            dims=['month', 'lev', 'lat', 'lon'],\n",
    "            name='SSAloss'\n",
    "        )\n",
    "        mock_climatology[p_type] = mock_da\n",
    "    \n",
    "    # Execute the plotting function with the mock data\n",
    "    plot_climatological_timeseries_comparison(mock_climatology)\n",
    "\n",
    "else:\n",
    "    # Execute the plotting function with the real data\n",
    "    plot_climatological_timeseries_comparison(SSAloss_monthly_climatology)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beab3ab9-25ad-4515-9eee-559f7c7cfa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "# --- GEOGRAPHIC FOCUS ---\n",
    "# Coordinates for Macquarie Island (as requested)\n",
    "TARGET_LAT = -54.5\n",
    "TARGET_LON = 158.95\n",
    "# ------------------------\n",
    "\n",
    "# --- NOTE TO USER: SETUP ASSUMPTION ---\n",
    "# This code assumes the 'SSAloss_monthly_climatology' dictionary has \n",
    "# already been created using the 'calculate_climatology.py' script \n",
    "# (4D DataArrays: month, lev, lat, lon).\n",
    "# -------------------------------------\n",
    "\n",
    "# Define all scenarios\n",
    "ALL_PERTURBATION_TYPES = ['NP', 'OL', 'OM', 'SSAC', 'SSA0', 'WL', 'WM']\n",
    "\n",
    "# Defining maps' colors, line style, and legend labels (as requested)\n",
    "colors = {\n",
    "    'NP': 'black', 'WM': 'blue', 'WL': 'lightblue',\n",
    "    'OM': 'green', 'OL': 'lightgreen', 'SSA0': 'red', 'SSAC': 'yellow'\n",
    "}\n",
    "\n",
    "linestyles = {\n",
    "    'NP': '--', 'WM': '-', 'WL': '-',\n",
    "    'OM': '-', 'OL': '-', 'SSA0': '-', 'SSAC': '-'\n",
    "}\n",
    "\n",
    "legend_map = {\n",
    "    'NP': 'No Perturbations',\n",
    "    'WM': 'More_Wind',\n",
    "    'WL': 'Less_Wind',\n",
    "    'OM': 'More_Ocean',\n",
    "    'OL': 'Less_Ocean',\n",
    "    'SSA0': 'Sea Salt Aerosols (Zero)',\n",
    "    'SSAC': 'Sea Salt Aerosols (Constant)'\n",
    "}\n",
    "\n",
    "# 1-character month abbreviations for X-axis labels (as requested)\n",
    "MONTH_ABBREVIATIONS = ['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D']\n",
    "\n",
    "\n",
    "def plot_climatological_timeseries_comparison(climatology_dict):\n",
    "    \"\"\"\n",
    "    Plots the 12-month climatological cycle for ALL specified scenarios \n",
    "    on a single graph, focused on the location defined by TARGET_LAT/LON.\n",
    "    \n",
    "    The data is summed vertically (lev) to represent the total atmospheric \n",
    "    impact at the single selected grid point.\n",
    "    \n",
    "    Args:\n",
    "        climatology_dict (dict): Dictionary containing the 4D monthly climatology DataArrays.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not all(p in climatology_dict for p in ALL_PERTURBATION_TYPES):\n",
    "        missing = [p for p in ALL_PERTURBATION_TYPES if p not in climatology_dict]\n",
    "        print(f\"Error: Missing cases in dictionary: {missing}. Cannot plot all series.\")\n",
    "        return\n",
    "\n",
    "    # 1. Setup the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    \n",
    "    # Get month coordinates (same for all cases)\n",
    "    first_case = ALL_PERTURBATION_TYPES[0]\n",
    "    months = climatology_dict[first_case]['month'].values\n",
    "    month_names = MONTH_ABBREVIATIONS\n",
    "    \n",
    "    plot_title = f\"Sea Salt Uptake - Comparison at Lat: {TARGET_LAT}, Lon: {TARGET_LON}\"\n",
    "\n",
    "    # 2. Loop through all perturbation types and plot the series\n",
    "    for p_type in ALL_PERTURBATION_TYPES:\n",
    "        data_4d = climatology_dict[p_type]\n",
    "        \n",
    "        # a. Select the single grid cell closest to the target coordinates\n",
    "        # Use method='nearest' to ensure we pick the closest grid point\n",
    "        data_point = data_4d.sel(\n",
    "            lat=TARGET_LAT, \n",
    "            lon=TARGET_LON, \n",
    "            method='nearest'\n",
    "        )\n",
    "        \n",
    "        # b. Sum across the vertical dimension ('lev') to get the Column Total at that point\n",
    "        # The result is a 1D array indexed by 'month'.\n",
    "        time_series_data = data_point.sum(dim='lev')\n",
    "        \n",
    "        # c. Plot the series using custom colors, linestyles, and legend labels\n",
    "        ax.plot(\n",
    "            months, \n",
    "            time_series_data.values, \n",
    "            marker='o', \n",
    "            linewidth=2,\n",
    "            color=colors.get(p_type, 'gray'),             \n",
    "            linestyle=linestyles.get(p_type, '-'),        \n",
    "            label=legend_map.get(p_type, p_type)          \n",
    "        )\n",
    "    \n",
    "    # 3. Final Plot Customization\n",
    "    \n",
    "    # Set X-axis to display 1-character month names\n",
    "    ax.set_xticks(months)\n",
    "    ax.set_xticklabels(month_names)\n",
    "    \n",
    "    # Labeling and Titling\n",
    "    ax.set_title(plot_title, fontsize=16, fontweight='bold')\n",
    "    ax.set_xlabel(\"Month of the Year\", fontsize=12)\n",
    "    ax.set_ylabel(\"Sea Salt Uptake (ug/m2*year)\", fontsize=12)\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "    \n",
    "    # Add Legend to distinguish the lines\n",
    "    ax.legend(title=\"Scenario\", frameon=True, shadow=True, loc='best')\n",
    "    \n",
    "    # Improve aesthetics\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- MOCKUP DATA FOR RUNNABILITY (Replace with your actual execution) ---\n",
    "# If your SSAloss_monthly_climatology is not defined, this block runs a small example\n",
    "if 'SSAloss_monthly_climatology' not in globals():\n",
    "    print(f\"\\n--- Running Mockup Example Focused on Lat: {TARGET_LAT}, Lon: {TARGET_LON} ---\")\n",
    "    \n",
    "    # Create mock 4D climatology data (12 months, 2 lev, 2 lat, 2 lon)\n",
    "    mock_months = np.arange(1, 13)\n",
    "    mock_lev = np.array([1000, 500])\n",
    "    \n",
    "    # Use coordinates near the target for the mock data\n",
    "    mock_lat = np.array([-54, -55])\n",
    "    mock_lon = np.array([158, 159])\n",
    "    \n",
    "    # Create synthetic seasonal data (peaks in Southern Hemisphere summer, Dec-Feb)\n",
    "    # Shifted cosine function to peak around month 1 (Jan)\n",
    "    base_seasonal = (np.cos(np.linspace(0, 2 * np.pi, 12) + 2.6) * 0.5 + 1.5)\n",
    "    \n",
    "    # Create the mock climatology dictionary with varied scales\n",
    "    mock_climatology = {}\n",
    "    \n",
    "    for i, p_type in enumerate(ALL_PERTURBATION_TYPES):\n",
    "        # Scale each type slightly differently\n",
    "        scale = 1e-4 * (1 + i * 0.1) \n",
    "        \n",
    "        # Expand data to 4D structure\n",
    "        # Add a slight spatial gradient so the selection is meaningful\n",
    "        spatial_gradient = np.array([[[1.1, 1.0], [0.9, 0.8]]]) * scale\n",
    "        mock_4d_data = base_seasonal[:, np.newaxis, np.newaxis, np.newaxis] * spatial_gradient\n",
    "        \n",
    "        mock_da = xr.DataArray(\n",
    "            mock_4d_data,\n",
    "            coords={'month': mock_months, 'lev': mock_lev, 'lat': mock_lat, 'lon': mock_lon},\n",
    "            dims=['month', 'lev', 'lat', 'lon'],\n",
    "            name='SSAloss'\n",
    "        )\n",
    "        mock_climatology[p_type] = mock_da\n",
    "    \n",
    "    # Execute the plotting function with the mock data\n",
    "    plot_climatological_timeseries_comparison(mock_climatology)\n",
    "\n",
    "else:\n",
    "    # Execute the plotting function with the real data\n",
    "    plot_climatological_timeseries_comparison(SSAloss_monthly_climatology)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebee192-0633-4a29-9ecf-c5ee23851878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f2da1-a4ff-42ba-8990-006787e32b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f058fb8-a2a2-4fd2-8275-497c2321fd1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a91383d-ce83-4a3d-9910-68a8cefc0ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abc193-268e-4606-9016-a4f114d9e292",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(monthly_means_by_type_HgChem_Hg2losstoSSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a45f3a-0383-4a9c-9ee3-dde7a4a2b340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumendo che il tuo dizionario sia già caricato in 'monthly_means_by_type_HgChem_Hg2losstoSSA'\n",
    "\n",
    "# 1. Definisci il nuovo dizionario per i risultati\n",
    "monthly_sumslev_by_type_HgChem_Hg2losstoSSA = {}\n",
    "\n",
    "# 2. Itera su ciascun elemento (chiave, DataArray)\n",
    "for tipo, data_array in monthly_means_by_type_HgChem_Hg2losstoSSA.items():\n",
    "    \n",
    "    # 3. Calcola la somma lungo la dimensione 'lev'\n",
    "    # .compute() è aggiunto se stai lavorando con dask array, per forzare l'esecuzione\n",
    "    sum_over_levels = data_array.sum(dim='lev') # .compute() \n",
    "    \n",
    "    # 4. Assegna il risultato al nuovo dizionario\n",
    "    monthly_sumslev_by_type_HgChem_Hg2losstoSSA[tipo] = sum_over_levels\n",
    "\n",
    "# Stampa il primo elemento per verificare il risultato\n",
    "print(monthly_sumslev_by_type_HgChem_Hg2losstoSSA['OM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028f8085-b295-47e8-8cc8-6a1e4bbba138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffbf2bd-9506-4ec0-8156-3c079a9670f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops: Datarrays for each perturbation type \n",
    "data_arrays_by_type_HgChem_Hg2losstoSSA = {u: [] for u in perturbation_type_map.values()}\n",
    "\n",
    "# Dictionary to save the final monthly and annual mean values \n",
    "\n",
    "monthly_means_by_type_HgChem_Hg2losstoSSA = {}\n",
    "annual_means_by_type_HgChem_Hg2losstoSSA = {}\n",
    "\n",
    "#### STANDARD DEVIATION\n",
    "\n",
    "monthly_std_by_type_HgChem_Hg2losstoSSA = {}\n",
    "annual_std_by_type_HgChem_Hg2losstoSSA = {}\n",
    "\n",
    "###########################################################################\n",
    "# First loop: Datarrays for each perturbation type \n",
    "\n",
    "for t, t_key in perturbation_type_map.items():\n",
    "    \n",
    "    for y in years:\n",
    "        \n",
    "        full_key_HgChem = f\"{t_key}_{y}_HgChem\"\n",
    "\n",
    "        #### HgChem \n",
    "        \n",
    "        if full_key_HgChem in datasets_by_year:\n",
    "            \n",
    "            data_array_HgChem_Hg2losstoSSA = datasets_by_year[full_key_HgChem]['Hg2GasToSSA']\n",
    "            data_arrays_by_type_HgChem_Hg2losstoSSA[t_key].append(data_array_HgChem_Hg2losstoSSA)\n",
    "        else:\n",
    "            print(f\"Warning: The key {full_key_HgChem} not found .\")\n",
    "\n",
    "#################################################################################\n",
    "# Second loop: Calculating the monthly and annual mean for each perturbation type \n",
    "\n",
    "###### HgChem Hg2GasToSSA\n",
    "\n",
    "for t_key, data_arrays_list_HgChem_Hg2losstoSSA in data_arrays_by_type_HgChem_Hg2losstoSSA.items():\n",
    "    \n",
    "    if data_arrays_list_HgChem_Hg2losstoSSA:\n",
    "        \n",
    "        combined_data_HgChem_Hg2losstoSSA = xr.concat(data_arrays_list_HgChem_Hg2losstoSSA, dim='time')\n",
    "        \n",
    "        # Calcolo della media annuale (_HgChem_H2losstoSSA)\n",
    "        \n",
    "        annual_mean_Hg2toSSA = combined_data_HgChem_Hg2losstoSSA.mean(dim='time', skipna=True)\n",
    "        annual_std_Hg2toSSA = combined_data_HgChem_Hg2losstoSSA.std(dim='time', skipna=True)\n",
    "        \n",
    "        annual_means_by_type_HgChem_Hg2losstoSSA[t_key] = annual_mean_Hg2toSSA\n",
    "        annual_std_by_type_HgChem_Hg2losstoSSA[t_key] = annual_std_Hg2toSSA\n",
    "        \n",
    "        print(f\"_HgChem_Hg2losstoSSA - Annual mean and std calculation for {t_key} completed.\")\n",
    "\n",
    "        # Calcolo della media mensile (_Hg2toSSA)\n",
    "        \n",
    "        monthly_mean_Hg2toSSA = combined_data_HgChem_Hg2losstoSSA.groupby('time.month').mean(dim='time', skipna=True)\n",
    "        monthly_std_Hg2toSSA = combined_data_HgChem_Hg2losstoSSA.groupby('time.month').std(dim='time', skipna=True)\n",
    "    \n",
    "        monthly_means_by_type_HgChem_Hg2losstoSSA[t_key] = monthly_mean_Hg2toSSA\n",
    "        monthly_std_by_type_HgChem_Hg2losstoSSA[t_key] = monthly_std_Hg2toSSA\n",
    "        \n",
    "        print(f\"_HgChem_Hg2losstoSSA - Monthly mean and std calculation for {t_key} completed.\")\n",
    "    else:\n",
    "        print(f\"_HgChem_Hg2losstoSSA - Nessun dato trovato per {t_key}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6101a8-c795-4a1b-a851-65aba8ebe342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
